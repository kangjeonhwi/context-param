import os
import json
import torch
import pandas as pd
from dataclasses import dataclass
from typing import List, Dict
from datasets import load_from_disk, Dataset, load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model, PeftModel
from accelerate import Accelerator
import gc



# ======================== ì„¤ì • ========================
@dataclass
class ExperimentConfig:
    model_id: str = "meta-llama/Llama-3.1-8B-Instruct"
    dataset_path: str = "/mnt/raid5/kangjh/Research/context-param/dataset/sample_dataset/eval.json"
    output_base_dir: str = "/mnt/raid5/kangjh/Research/context-param/Exp1/adapters"
    metadata_file: str = "./adapter_metadata.json"
    
    num_samples: int = 200
    sample_stride: int = 1
    
    # LoRA ì„¤ì •
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.05
    target_modules: List[str] = None
    
    # í•™ìŠµ ì„¤ì •
    learning_rate: float = 2e-4
    max_context_length: int = 4096
    per_device_train_batch_size: int = 1
    
    # ì‹¤í—˜ ì¡°í•©
    epoch_settings: List[int] = None
    adapter_types: List[str] = None
    training_prompt_settings: List[bool] = None
    loss_masking_settings: List[str] = None
    
    train_prompt_prefix: str = "Please memorize the following context carefully and answer the question based on it. Context: "
    
    def __post_init__(self):
        if self.target_modules is None:
            self.target_modules = ["gate_proj", "up_proj", "down_proj"]
        if self.epoch_settings is None:
            self.epoch_settings = [3, 7, 15]
        if self.adapter_types is None:
            self.adapter_types = ["LoRA", "DoRA"]
        if self.training_prompt_settings is None:
            self.training_prompt_settings = [False, True]
        if self.loss_masking_settings is None:
            self.loss_masking_settings = ["all", "context_only"]



# ======================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ========================
def prepare_training_data(
    sample: Dict,
    tokenizer,
    config: ExperimentConfig,
    use_train_prompt: bool,
    loss_masking_strategy: str
) -> Dict:
    """
    ë‹¨ì¼ ìƒ˜í”Œì— ëŒ€í•œ í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    """
    context = sample['context']
    
    if use_train_prompt:
        training_content = f"{config.train_prompt_prefix}\n{context}"
        tokenized_data = tokenizer(
            training_content, 
            truncation=True, 
            max_length=config.max_context_length
        )
        labels = tokenized_data['input_ids'].copy()
        
        # Context-only loss masking
        if loss_masking_strategy == "context_only":
            prompt_only_tokens = tokenizer(
                f"{config.train_prompt_prefix}\n", 
                add_special_tokens=True
            )
            prompt_len_with_bos = len(prompt_only_tokens['input_ids'])
            for k in range(prompt_len_with_bos):
                if k < len(labels):
                    labels[k] = -100
        
        train_data_dict = {
            "input_ids": tokenized_data['input_ids'],
            "attention_mask": tokenized_data['attention_mask'],
            "labels": labels
        }
    else:  # Naive: loss on context only
        tokenized_data = tokenizer(
            context, 
            truncation=True, 
            max_length=config.max_context_length
        )
        train_data_dict = {
            "input_ids": tokenized_data['input_ids'],
            "attention_mask": tokenized_data['attention_mask'],
            "labels": tokenized_data['input_ids'].copy()
        }
    
    return train_data_dict



def safe_delete_adapter(model: PeftModel, adapter_name: str = None):
    """
    LoRA ì–´ëŒ‘í„°ë¥¼ ì•ˆì „í•˜ê²Œ ì œê±°í•˜ê³  ë©”ëª¨ë¦¬ í•´ì œ
    """
    try:
        if adapter_name:
            # íŠ¹ì • ì–´ëŒ‘í„° ì‚­ì œ
            if hasattr(model, 'delete_adapter'):
                model.delete_adapter(adapter_name)
        else:
            # ëª¨ë“  ì–´ëŒ‘í„° unload
            if hasattr(model, 'unload'):
                model.unload()
        
        # ëª…ì‹œì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âš ï¸ ì–´ëŒ‘í„° ì‚­ì œ ì¤‘ ê²½ê³ : {e}")



def get_experiment_path(
    base_dir: str,
    sample_idx: int,
    adapter_type: str,
    num_epochs: int,
    use_train_prompt: bool,
    loss_masking_strategy: str
) -> tuple[str, str]:
    """
    ê¹”ë”í•œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±: sample{idx}/{adapter_type}/e{epochs}_{suffix}/
    
    Returns:
        (output_dir, experiment_key)
    """
    # Suffix ìƒì„±
    if not use_train_prompt:
        suffix = "naive"
    else:
        suffix = f"prompted_loss_{loss_masking_strategy}"
    
    # ê³„ì¸µì  ë””ë ‰í† ë¦¬ êµ¬ì¡°
    adapter_dir = adapter_type.lower()
    epoch_config_dir = f"e{num_epochs}_{suffix}"
    
    output_dir = os.path.join(
        base_dir,
        f"sample{sample_idx}",
        adapter_dir,
        epoch_config_dir
    )
    
    # ì‹¤í—˜ í‚¤ (ë©”íƒ€ë°ì´í„°ìš©)
    experiment_key = f"sample{sample_idx}_{adapter_dir}_e{num_epochs}_{suffix}"
    
    return output_dir, experiment_key



# ======================== ë©”ì¸ í•™ìŠµ í•¨ìˆ˜ ========================
def train_single_adapter(
    base_model,
    tokenizer,
    train_dataset: Dataset,
    config: ExperimentConfig,
    adapter_type: str,
    num_epochs: int,
    sample_idx: int,
    use_train_prompt: bool,
    loss_masking_strategy: str,
    accelerator: Accelerator
) -> Dict:
    """
    ë‹¨ì¼ LoRA ì–´ëŒ‘í„° í•™ìŠµ ë° ì €ì¥
    """
    # ì¶œë ¥ ê²½ë¡œ ìƒì„± (ìƒˆë¡œìš´ ê³„ì¸µì  êµ¬ì¡°)
    output_dir, experiment_key = get_experiment_path(
        config.output_base_dir,
        sample_idx,
        adapter_type,
        num_epochs,
        use_train_prompt,
        loss_masking_strategy
    )
    
    print(f"\n{'='*70}")
    print(f"ğŸ”§ í•™ìŠµ ì‹œì‘: {experiment_key}")
    print(f"   ğŸ“ ì €ì¥ ê²½ë¡œ: {output_dir}")
    print(f"{'='*70}")
    
    # LoRA Config ìƒì„±
    peft_config = LoraConfig(
        r=config.lora_r,
        lora_alpha=config.lora_alpha,
        target_modules=config.target_modules,
        lora_dropout=config.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
        use_dora=(adapter_type == "DoRA")
    )
    
    # PEFT ëª¨ë¸ ìƒì„±
    peft_model = get_peft_model(base_model, peft_config)
    peft_model.print_trainable_parameters()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # Training Arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=config.per_device_train_batch_size,
        num_train_epochs=num_epochs,
        learning_rate=config.learning_rate,
        logging_steps=1,
        save_strategy="no",
        save_total_limit=0,
        report_to="none",
        remove_unused_columns=False,
        fp16=False,
        bf16=True,

        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False},
    )
    
    # Data Collator
    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
    
    # Trainer
    trainer = Trainer(
        model=peft_model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=data_collator,
    )
    
    # í•™ìŠµ ì‹¤í–‰
    trainer.train()
    
    # Loss ì •ë³´ ì¶”ì¶œ
    log_history = trainer.state.log_history
    losses = [log['loss'] for log in log_history if 'loss' in log]
    
    loss_stats = {
        "max_loss": float(max(losses)) if losses else None,
        "min_loss": float(min(losses)) if losses else None,
        "avg_loss": float(sum(losses) / len(losses)) if losses else None,
        "final_loss": float(losses[-1]) if losses else None,
    }
    
    # ì–´ëŒ‘í„° ì €ì¥
    adapter_save_path = os.path.join(output_dir, "adapter")
    peft_model.save_pretrained(adapter_save_path)
    print(f"âœ… ì–´ëŒ‘í„° ì €ì¥ ì™„ë£Œ: {adapter_save_path}")
    
    # ë©”íƒ€ë°ì´í„° ìƒì„±
    metadata = {
        "experiment_key": experiment_key,
        "adapter_path": adapter_save_path,
        "output_dir": output_dir,
        "adapter_type": adapter_type,
        "num_epochs": num_epochs,
        "loss_stats": loss_stats,
    }
    
    # ê°œë³„ ë©”íƒ€ë°ì´í„° ì €ì¥
    metadata_path = os.path.join(output_dir, "metadata.json")
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    # ì–´ëŒ‘í„° unload
    print(f"ğŸ§¹ ì–´ëŒ‘í„° unload ì¤‘...")
    safe_delete_adapter(peft_model)
    
    del peft_model
    del trainer
    gc.collect()
    torch.cuda.empty_cache()
    
    print(f"âœ… í•™ìŠµ ì™„ë£Œ ë° ë©”ëª¨ë¦¬ ì •ë¦¬: {experiment_key}\n")
    
    return metadata



def main():
    print("\n" + "="*80)
    print("ğŸš€ LoRA ì–´ëŒ‘í„° í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (Accelerate í™œìš©)")
    print("="*80 + "\n")
    
    # Config ë¡œë“œ
    config = ExperimentConfig()
    
    # Accelerator ì´ˆê¸°í™”
    accelerator = Accelerator()
    print(f"ğŸ“Š Accelerator ì •ë³´:")
    print(f"  - ì‚¬ìš© ê°€ëŠ¥ GPU: {torch.cuda.device_count()}")
    print(f"  - í˜„ì¬ í”„ë¡œì„¸ìŠ¤: {accelerator.process_index}/{accelerator.num_processes}")
    print(f"  - Device: {accelerator.device}\n")
    
    # Base Model & Tokenizer ë¡œë“œ
    print(f"ğŸ”„ Base Model ë¡œë“œ ì¤‘: {config.model_id}")
    base_model = AutoModelForCausalLM.from_pretrained(
        config.model_id,
        device_map={"": accelerator.device},
        torch_dtype=torch.bfloat16,
        attn_implementation="flash_attention_2"
    )
    base_model.config.use_cache = False
    
    tokenizer = AutoTokenizer.from_pretrained(config.model_id)
    tokenizer.pad_token = tokenizer.eos_token
    
    print(f"âœ… Base Model ë¡œë“œ ì™„ë£Œ\n")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {config.dataset_path}")
    dataset = load_dataset(
        "json", 
        data_files=config.dataset_path, 
        field="data"
    )['train']
    print(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)} ìƒ˜í”Œ\n")
    
    # ë©”íƒ€ë°ì´í„° ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    all_metadata = []
    
    # ìƒ˜í”Œë³„ í•™ìŠµ ë£¨í”„
    for sample_idx in range(config.num_samples):
        actual_idx = sample_idx * config.sample_stride
        sample = dataset[actual_idx]
        sample_id = sample['context_id']
        
        print(f"\n{'#'*80}")
        print(f"ğŸ“ ìƒ˜í”Œ {sample_idx}/{config.num_samples} ì²˜ë¦¬ ì¤‘ (ì‹¤ì œ ì¸ë±ìŠ¤: {actual_idx})")
        print(f"{'#'*80}")
        
        # ì‹¤í—˜ ì¡°í•©ë³„ í•™ìŠµ
        for use_train_prompt in config.training_prompt_settings:
            for loss_masking_strategy in config.loss_masking_settings:
                
                # Naive ì„¤ì • ì‹œ context_onlyëŠ” ìŠ¤í‚µ
                if not use_train_prompt and loss_masking_strategy == "context_only":
                    continue
                
                # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
                train_data_dict = prepare_training_data(
                    sample, tokenizer, config,
                    use_train_prompt, loss_masking_strategy
                )
                train_dataset = Dataset.from_dict({
                    k: [v] for k, v in train_data_dict.items()
                })
                
                # Adapter Type & Epochs ì¡°í•©
                for adapter_type in config.adapter_types:
                    if adapter_type == "DoRA" and use_train_prompt :
                        continue

                    for num_epochs in config.epoch_settings:
                        
                        # Base model ìƒíƒœ ì²´í¬
                        if isinstance(base_model, PeftModel):
                            print(f"âš ï¸ ê²½ê³ : Base modelì— ì–´ëŒ‘í„°ê°€ ë‚¨ì•„ìˆìŒ. ê°•ì œ ì •ë¦¬ ì¤‘...")
                            safe_delete_adapter(base_model)
                        
                        # í•™ìŠµ ì‹¤í–‰
                        try:
                            metadata = train_single_adapter(
                                base_model, tokenizer, train_dataset,
                                config, adapter_type, num_epochs,
                                actual_idx, use_train_prompt, loss_masking_strategy,
                                accelerator
                            )
                            
                            # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
                            metadata['context_id'] = sample_id
                            metadata['use_train_prompt'] = use_train_prompt
                            metadata['loss_masking_strategy'] = loss_masking_strategy
                            
                            all_metadata.append(metadata)
                            
                        except Exception as e:
                            print(f"ğŸš¨ ì—ëŸ¬ ë°œìƒ: sample{actual_idx}_{adapter_type}_e{num_epochs}")
                            print(f"   {str(e)}")
                            gc.collect()
                            torch.cuda.empty_cache()
                            continue
    
    # ì „ì²´ ë©”íƒ€ë°ì´í„° ì €ì¥
    print(f"\n{'='*80}")
    print(f"ğŸ’¾ ì „ì²´ ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘...")
    with open(config.metadata_file, 'w', encoding='utf-8') as f:
        json.dump(all_metadata, f, indent=2, ensure_ascii=False)
    print(f"âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {config.metadata_file}")
    
    # CSV ìš”ì•½ ì €ì¥
    df = pd.DataFrame(all_metadata)
    summary_csv = config.metadata_file.replace('.json', '_summary.csv')
    df.to_csv(summary_csv, index=False, encoding='utf-8-sig')
    print(f"âœ… ìš”ì•½ CSV ì €ì¥ ì™„ë£Œ: {summary_csv}")
    
    print(f"\n{'='*80}")
    print(f"ğŸ‰ ì „ì²´ í•™ìŠµ ì™„ë£Œ!")
    print(f"   - ì´ ì–´ëŒ‘í„° ìˆ˜: {len(all_metadata)}")
    print(f"   - ì €ì¥ ìœ„ì¹˜: {config.output_base_dir}")
    print(f"{'='*80}\n")



if __name__ == "__main__":
    main()